head(xy.pca)
fit <- lm(y~., data=xy.pca)
summary(fit)
summary(fit)
x.test.pca  <- predict(pca.out, x.test)[,c(1:5)]
xy.test.pca <- as.data.frame(cbind(x.test.pca, y.test))
y.pred <- predict(fit, xy.test.pca)
se <- (y.pred- y.test)^2
sum(se)/1000
mean(se)
train <- as.data.frame(cbind(x, y))
train <- as.data.frame(cbind(x, y))
test  <- as.data.frame(cbind(x.test, y.test))
fit.all <- lm(y~., train)
y.all.pred <- predict(fit.all, test)
se.all <- (y.all.pred -y.test)^2
mean(se.all)
load(file)
train <- as.data.frame(cbind(x, y))
test  <- as.data.frame(cbind(x.test, y.test))
fit.all <- lm(y~., train)
y.all.pred <- predict(fit.all, test)
se.all <- (y.all.pred -y.test)^2
mean(se.all)
file <- "C:\\Users\\mmorelli\\Google Drive\\Statistical Learning Stanford\\Week10\\10.R.RData"
load(file)
train <- as.data.frame(cbind(x, y))
test  <- as.data.frame(cbind(x.test, y.test))
fit.all <- lm(y~., train)
y.all.pred <- predict(fit.all, test)
se.all <- (y.all.pred -y.test)^2
mean(se.all)
?lm
fit.all
summary(fit.all)
mean(se.all)
file <- "C:\\Users\\mmorelli\\Documents\\work\\Campagne\\10.R.RData\\campagne.csv"
camp <- load.csv(file)
?load
camp <- read.csv(file)
file <- "C:\\Users\\mmorelli\\Documents\\work\\Campagne\\campagne.csv"
camp <- read.csv(file)
head(camp)
names(camp)
?read.csv
camp <- read.csv(file, sep=";")
head(camp)
names(camp)
unique(Subject)
unique(camp$Subject)
names(camp)
unique(camp$Email.Template.Name)
camp[leistung$Email.Template.Name %in% c("2016-01 Contacts Compliance", "2016-01 Mailing Compliance", "2016-01 Leads Compliance"),]$template <- "Compliance"
camp[leistung$Email.Template.Name %in% c("2016-01 Contacts Schadensvermeidung", "2016-01 Leads Schadensvermeidung", "2016-01 Mailing Schadensvermeidung"),]$template <- "Schadensverm."
camp[leistung$Email.Template.Name %in% c("2016-01 Contacts Zeitoptimierung", "2016-01 Mailing Zeitoptimierung", "2016-01 Leads Zeitoptimierung"),]$template <- "Zeitoptim"
camp[leistung$Email.Template.Name %in% c("2016-01 Contacts _berblick", "2016-01 Mailing _berblick", "2016-01 Leads _berblick"),]$template <- ""
camp[leistung$Email.Template.Name %in% c("2016-01 Contacts Planungssicherheit", "2016-01 Mailing Planungssicherheit", "2016-01 Leads Planungssicherheit"),]$template <- ""
camp[camp$Email.Template.Name %in% c("2016-01 Contacts Compliance", "2016-01 Mailing Compliance", "2016-01 Leads Compliance"),]$template <- "Compliance"
camp[camp$Email.Template.Name %in% c("2016-01 Contacts Compliance", "2016-01 Mailing Compliance", "2016-01 Leads Compliance"),]
names(camp)
histogram(camp$OpenedNum)
library(ggplot2)
m <- ggplot(camp, aes(x=OpenedNum))
m + geom_histogram()
str(camp)
m <- ggplot(camp, aes(x=X..Times.Opened))
m + geom_histogram()
m <- ggplot(camp, aes(x=Pattern))
m + geom_histogram()
m <- ggplot(camp, aes(x=X..Times.Opened))
m + geom_histogram()
tail(camp)
camp[is.na(camp[,"Count"]),]
camp[!is.na(camp[,"Count"]),]
camp <- camp[!is.na(camp[,"Count"]),]
m <- ggplot(camp, aes(x=X..Times.Opened))
m + geom_histogram()
?table
names(camp)
table(camp$X..Times.Opened, camp$Type)
table(camp$X..Times.Opened)
table(camp$X..Times.Opened, camp$Type)
names(camp)
table(camp$Wann.Gen, camp$Type)
table(camp[camp$Genervt == 1,]$Wann.Gen, camp[camp$Genervt == 1,]$$Type)
table(camp[camp$Genervt == 1,]$Wann.Gen, camp[camp$Genervt == 1,]$Type)
head(camp)
str(camp)
m <- ggplot(camp, aes(x=X..Times.Opened)) + geom_histogram()
m
table(camp[camp$Genervt == "Ok",]$Wann.Gen, camp[camp$Genervt == "Ok",]$Type)
table(camp[camp$Genervt == "Gen",]$Wann.Gen, camp[camp$Genervt == "Gen",]$Type)
table(camp[camp$Genervt == "Ok",]$Rehienfolge, camp[camp$Genervt == "Ok",]$Type)
str(camp)
table(camp[camp$Genervt == "Ok",]$Rehienfolge, camp[camp$Genervt == "Ok",]$Type)
camp[camp$Genervt == "Ok",]$Rehienfolge
camp[camp$Genervt == "Ok",]$Rehienfolge
camp$Genervt
camp$Genervt == "Ok"
camp[camp$Genervt == "Ok",]
table(camp[camp$Genervt == "Ok",]$Reihenfolge, camp[camp$Genervt == "Ok",]$Type)
file <- "C:\\Users\\mmorelli\\Google Drive\\Statistical Learning Stanford\\Week10\\10.R.RData"
load(file)
head(x)
tot <- rbind(x, x.test)
pca.out=prcomp(tot, scale=TRUE)
pca.out$sdev
redux <- pca.out$x[1,c(1,5)]
dim(redux)
summary(redux)
dimension(redux)
str(redux)
dim.data.frame(redux)
length(redux)
nrow(redux)
redux
redux <- pca.out$x[,c(1,5)]
redux <- pca.out$x[,c(1,5)]
dim(redux)
file <- "C:\\Users\\mmorelli\\Google Drive\\Statistical Learning Stanford\\Week10\\10.R.RData"
load(file)
head(x)
tot <- rbind(x, x.test)
pca.out=prcomp(tot, scale=TRUE)
pca.out$sdev
redux <- pca.out$x[,c(1,5)]
dim(redux)
x.pca  <-predict(pca.out, x)[,c(1:5)]
file <- "C:\\Users\\mmorelli\\Google Drive\\Statistical Learning Stanford\\Week10\\10.R.RData"
load(file)
tot <- rbind(x, x.test)
pca.out=prcomp(tot, scale=TRUE)
file <- "C:\\Users\\mmorelli\\Google Drive\\Statistical Learning Stanford\\Week10\\10.R.RData"
load(file)
tot <- rbind(x, x.test)
pca.out=prcomp(tot, scale=TRUE)
pca.out$sdev
x.pca  <-predict(pca.out, x)[,c(1:5)]
Group<-c(rep("Frank",times=6),rep("Greg",times=11),rep("Stacy",times=3),rep("Nancy",times=10))
X<-c(4,5,3,5,7,4,8,23,4,7,5,2,8,5,8,3,6,5,4,6,8,9,2,5,8,3,6,3,3,4)
Y<-c(7,9,3,6,4,8,7,8,6,3,2,3,6,7,4,6,8,9,5,7,8,9,6,5,4,6,7,8,3,6)
df<-data.frame(Group,as.numeric(X),as.numeric(Y))
Group
df
library(geometry)
install.package(geometry)
install.packages("geometry")
library(geometry)
convhulln(Frank.frame, option="FA")$vol
Frank.frame<-cbind(df$X[df$Group=="Frank"],df$Y[df$Group=="Frank"])
convhulln(Frank.frame, option="FA")$vol
setwd("C:\\Users\\mmorelli\\Google Drive\\MITx 15 071x The Analytics Edge\\Week 03")
# Unit 3, The Framingham Heart Study
# Video 3
# Read in the dataset
framingham = read.csv("framingham.csv")
# Look at structure
str(framingham)
# Load the library caTools
library(caTools)
# Randomly split the data into training and testing sets
set.seed(1000)
split = sample.split(framingham$TenYearCHD, SplitRatio = 0.65)
# Split up the data using subset
train = subset(framingham, split==TRUE)
test = subset(framingham, split==FALSE)
framinghamLog = glm(TenYearCHD ~ ., data = train, family=binomial)
summary(framinghamLog)
# Predictions on the test set
predictTest = predict(framinghamLog, type="response", newdata=test)
# Confusion matrix with threshold of 0.5
table(test$TenYearCHD, predictTest > 0.5)
# Accuracy
(1069+11)/(1069+6+187+11)
# Baseline accuracy
(1069+6)/(1069+6+187+11)
# Test set AUC
library(ROCR)
ROCRpred = prediction(predictTest, test$TenYearCHD)
as.numeric(performance(ROCRpred, "auc")@y.values)
table(test$TenYearCHD, predictTest > 0.5)
TP <- 11
TN <- 1069
FP <- 6
FN <- 11
?confusion
Sensitivity <- TP / (TP+FN)
Specificity <- Tn / (TN+FP)
Specificity <- TN / (TN+FP)
Sensitivity
Specificity
FN <- 187
Sensitivity <- TP / (TP+FN)
Specificity <- TN / (TN+FP)
Sensitivity
Specificity
setwd("C:\\Users\\mmorelli\\Google Drive\\MITx 15 071x The Analytics Edge\\Week 03")
# Unit 3, Recitation
# Video 2
# Read in data
polling = read.csv("PollingData.csv")
str(polling)
table(polling$Year)
summary(polling)
# Install and load mice package
install.packages("mice")
library(mice)
# Multiple imputation
simple = polling[c("Rasmussen", "SurveyUSA", "PropR", "DiffCount")]
summary(simple)
set.seed(144)
imputed = complete(mice(simple))
summary(imputed)
polling$Rasmussen = imputed$Rasmussen
polling$SurveyUSA = imputed$SurveyUSA
summary(polling)
?complete
setwd("C:\\Users\\mmorelli\\Google Drive\\MITx 15 071x The Analytics Edge\\Week 03")
songs <- read.csv("songs.csv")
summary(songs)
str(songs)
table(songs$year)
table(songs$artistname)
filter(songs, songs$artistname="Michael Jackson")
filter(songs, songs$artistname=="Michael Jackson")
songs[songs$artistname=="Michael Jackson",]
str(songs)
songs[songs$artistname=="Michael Jackson",c("songtitle", "Top10")]
(songs$timesignature)
unique(songs$timesignature)
hist(songs$timesignature)
order(songs, songs$tempo, decreasing=TRUE)
head(songs[ order(-songs[,"tempo"]), ])
setwd("C:\\Users\\mmorelli\\Google Drive\\MITx 15 071x The Analytics Edge\\Week 03")
songs <- read.csv("songs.csv")
str(songs)
train <- songs[songs$year <= 2009,]
test  <- songs[songs$year > 2009,]
nonvars <- c("year", "songtitle", "artistname", "songID", "artistID")
SongsTrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ]
SongsTest = SongsTest[ , !(names(SongsTest) %in% nonvars) ]
SongsTrain <- songs[songs$year <= 2009,]
SongsTest  <- songs[songs$year > 2009,]
nonvars <- c("year", "songtitle", "artistname", "songID", "artistID")
SongsTrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ]
SongsTest = SongsTest[ , !(names(SongsTest) %in% nonvars) ]
SongsLog1 = glm(Top10 ~ ., data=SongsTrain, family=binomial)
summary(SongsLog1)
cor(SongsTrain$energy, SongsTrain$loudness)
SongsLog2 = glm(Top10 ~ . -loudness, data=SongsTrain[,], family=binomial)
SongsLog3 = glm(Top10 ~ . -energy, data=SongsTrain, family=binomial)
summary(SongsLog2)
summary(SongsLog3)
summary(SongsLog2)
summary(SongsLog2)
summary(SongsLog3)
summary(SongsLog1)
TestPrediction = predict(SongsLog3, newdata=SongsTest, type="response")
table(SongsTest$Top10 , TestPrediction >= 0.5)
nrow(TestPrediction)
Accuracy <- (312+13)/nrow(SongsTest)
Accuracy
table(SongsTest$Top10 , TestPrediction >= 0.45)
Accuracy <- (309+19)/nrow(SongsTest)
Accuracy
1-sum(SongsTest$Top10)/nrow(SongsTest)
table(SongsTest$Top10 , TestPrediction >= 0.45)
TP <- 19
TN <- 309
FP <- 5
FN <- 40
Sensitivity <- TP / (TP+FN)
Specificity <- TN / (TN+FP)
Sensitivity
Specificity
parole <- read.csv("parole.csv")
sum(parole$violator)
summary(parole)
str(parole)
parole$state <- as.factor(parole$state)
parole$crime <- as.factor(parole$crime)
str(parole)
str(parole$state)
table(parole$state)
summary(parole)
library(caTools)
split = sample.split(parole$violator, SplitRatio = 0.7)
set.seed(144)
library(caTools)
split = sample.split(parole$violator, SplitRatio = 0.7)
train = subset(parole, split == TRUE)
summary(parole)
model1 = glm(violator ~ ., data=train, family=binomial)
summary(model1)
logit <- -4.2411574 + 0.3869904 + 0.8867192 + 50*(-0.0001756) + 3* (-0.1238867) + 12*0.0802954 + 0.6837143
odds <- exp(logit)
odds
+
.
P <- 1/(1+exp(-logit))
P
TestPrediction = predict(model1, newdata=test, type="response")
TestPrediction <- predict(model1, newdata=test, type="response")
summary(model1)
set.seed(144)
library(caTools)
split = sample.split(parole$violator, SplitRatio = 0.7)
train = subset(parole, split == TRUE)
test = subset(parole, split == FALSE)
model1 = glm(violator ~ ., data=train, family=binomial)
summary(model1)
TestPrediction <- predict(model1, newdata=test, type="response")
max(TestPrediction)
table(test$violator , TestPrediction >= 0.5)
Accuracy <- (167+12)/nrow(test)
Accuracy
TP <- 12
TN <- 167
FP <- 12
FN <- 11
Sensitivity <- TP / (TP+FN)
Specificity <- TN / (TN+FP)
Sensitivity
Specificity
1-sum(test$violator)/nrow(test)
1-sum(test$violator)/nrow(test)
library(ROCR)
ROCRpred = prediction(test, test$violator)
ROCRpred = prediction(TestPrediction, test$violator)
ROCRpred
as.numeric(performance(ROCRpred, "auc")@y.values)
loans <- read.csv("loans.csv")
summary(loans)
source('C:/Users/mmorelli/Google Drive/MITx 15 071x The Analytics Edge/Week 03/ES3.R', echo=TRUE)
str(loans)
str(loans)
loans <- read.csv("loans.csv")
str(loans)
mean(loans$not.fully.paid)
summary(loans)
library(mice)
set.seed(144)
vars.for.imputation = setdiff(names(loans), "not.fully.paid")
imputed = complete(mice(loans[vars.for.imputation]))
loans[vars.for.imputation] = imputed
set.seed(144)
library(caTools)
split = sample.split(loans$not.fully.paid, SplitRatio = 0.7)
train = subset(loans, split == TRUE)
test  = subset(loans, split == FALSE)
loans <- read.csv("loans.csv")
summary(loans)
str(loans)
mean(loans$not.fully.paid)
library(mice)
set.seed(144)
vars.for.imputation = setdiff(names(loans), "not.fully.paid")
imputed = complete(mice(loans[vars.for.imputation]))
loans[vars.for.imputation] = imputed
# --
set.seed(144)
library(caTools)
split = sample.split(loans$not.fully.paid, SplitRatio = 0.7)
train = subset(loans, split == TRUE)
test  = subset(loans, split == FALSE)
model1 <- glm(not.fully.paid . ~, data=loans, family=binomial )
model1 <- glm(not.fully.paid ~ ., data=loans, family=binomial )
summary(model1)
exp(9.054e-02)
set.seed(144)
split = sample.split(loans$not.fully.paid, SplitRatio = 0.7)
train = subset(loans, split == TRUE)
test  = subset(loans, split == FALSE)
model1 <- glm(not.fully.paid ~ ., data=loans, family=binomial )
summary(model1)
loans <- read.csv("loans_imputed.csv")
library(caTools)
loans <- read.csv("loans_imputed.csv")
set.seed(144)
split = sample.split(loans$not.fully.paid, SplitRatio = 0.7)
train = subset(loans, split == TRUE)
test  = subset(loans, split == FALSE)
model1 <- glm(not.fully.paid ~ ., data=loans, family=binomial )
summary(model1)
model1 <- glm(not.fully.paid ~ ., data=train, family=binomial )
summary(model1)
TestPrediction <- predict(model1, newdata=test, type="response")
test$predicted.risk <- TestPrediction
table(test$not.fully.paid , test$predicted.risk >= 0.5)
Accuracy <- (2400+3)/nrow(test)
Accuracy
1-sum(test$not.fully.paid)/nrow(test)
ROCRpred = prediction(test$predicted.risk, test$not.fully.paid)
as.numeric(performance(ROCRpred, "auc")@y.values)
View(test)
model.baseline <- glm(not.fully.paid ~ int.rate, data=train, family=binomial )
summary(model.baseline)
TestPredBaseline <- predict(model.baseline, newdata=test, type="response")
test$baseline <- TestPredBaseline
max(TestPredBaseline)
table(test$not.fully.paid , test$baseline >= 0.5)
ROCR.base = prediction(test$baseline, test$not.fully.paid)
as.numeric(performance(ROCR.base, "auc")@y.values)
C <- 10
R <- 0.06
t <- 3
C * exp(R*t)
test$profit = exp(test$int.rate*3) - 1
test$profit[test$not.fully.paid == 1] = -1
max(test$profit)
highInterest  <- subset(test, interest >= 0.15)
highInterest  <- subset(test, test$interest >= 0.15)
test$interest
highInterest  <- subset(test, test$int.rate >= 0.15)
mean(highInterest$profit)
sum(highInterest$not.fully.paid)/nrow(highInterest)
cutoff = sort(highInterest$predicted.risk, decreasing=FALSE)[100]
sort(highInterest$predicted.risk, decreasing=FALSE)[1:100]
best100  <- subset(highInterest, highInterest$predicted.risk <= cutoff)
mean(best100$profit)
sum(best100$not.fully.paid)
setwd("C:\\Users\\mmorelli\\Google Drive\\MITx 15 071x The Analytics Edge\\Week 03")
# Unit 3, The Framingham Heart Study
# Video 3
# Read in the dataset
framingham = read.csv("framingham.csv")
# Look at structure
str(framingham)
# Load the library caTools
library(caTools)
# Randomly split the data into training and testing sets
set.seed(1000)
split = sample.split(framingham$TenYearCHD, SplitRatio = 0.65)
# Split up the data using subset
train = subset(framingham, split==TRUE)
test  = subset(framingham, split==FALSE)
# Logistic Regression Model
framinghamLog = glm(TenYearCHD ~ ., data = train, family=binomial)
summary(framinghamLog)
# Predictions on the test set
predictTest = predict(framinghamLog, type="response", newdata=test)
# Confusion matrix with threshold of 0.5
table(test$TenYearCHD, predictTest > 0.5)
# Accuracy
(1069+11)/(1069+6+187+11)
# Baseline accuracy
(1069+6)/(1069+6+187+11)
# Test set AUC
library(ROCR)
ROCRpred = prediction(predictTest, test$TenYearCHD)
as.numeric(performance(ROCRpred, "auc")@y.values)
#------es
TP <- 11
TN <- 1069
FP <- 6
FN <- 187
Sensitivity <- TP / (TP+FN)
Specificity <- TN / (TN+FP)
setwd("C:\\Users\\mmorelli\\Google Drive\\MITx 15 071x The Analytics Edge\\Week 03")
# Unit 3, Modeling the Expert
# Video 4
# Read in dataset
quality = read.csv("quality.csv")
# Look at structure
str(quality)
# poor care is the target
# memberid key
# Table outcome
table(quality$PoorCare)
# Baseline accuracy
98/131
# Install and load caTools package
install.packages("caTools")
library(caTools)
# Randomly split data
set.seed(88)
split = sample.split(quality$PoorCare, SplitRatio = 0.75)
install.packages("caTools")
split
setwd("C:\\Users\\mmorelli\\Google Drive\\MITx 15 071x The Analytics Edge\\Week 03")
# Unit 3, Modeling the Expert
# Video 4
# Read in dataset
quality = read.csv("quality.csv")
# Look at structure
str(quality)
# memberid key
# poor care is the target
# Table outcome
table(quality$PoorCare)
# Baseline accuracy
98/131
library(caTools)
# Randomly split data
set.seed(88)
split = sample.split(quality$PoorCare, SplitRatio = 0.75)
split
# Create training and testing sets
qualityTrain = subset(quality, split == TRUE)
qualityTest = subset(quality, split == FALSE)
# Logistic Regression Model
QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family=binomial)
summary(QualityLog)
predictTest = predict(QualityLog, type="response", newdata=qualityTest)
ROCRpredTest = prediction(predictTest, qualityTest$PoorCare)
ROCRpredTest = predict(predictTest, qualityTest$PoorCare)
library(caTools)
ROCRpredTest = prediction(predictTest, qualityTest$PoorCare)
library(ROCR)
ROCRpredTest = prediction(predictTest, qualityTest$PoorCare)
auc = as.numeric(performance(ROCRpredTest, "auc")@y.values)
auc
