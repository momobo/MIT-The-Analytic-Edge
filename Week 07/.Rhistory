x2  = rnorm(N/2, 1, 1),
x3  = rnorm(N/2, 1, 1),
x4  = rnorm(N/2, 1, 1),
x5  = rnorm(N/2, 1, 1),
x6  = rnorm(N/2, 0, 1),
x7  = rnorm(N/2, 0, 1),
x8  = rnorm(N/2, 0, 1),
x9  = rnorm(N/2, 0, 1),
x10 = rnorm(N/2, 0, 1)
)
)
}
library(e1071)
train <- get_N(100)
TRY <- 100
svmfit<-svm(y~.,data=train,kernel="linear",cost=10,scale=FALSE)
test <- get_N(10000)
predicted <- predict(svmfit, newdata=test)
predicted
predicted>0.5
(predicted>0.5) = test$y
(predicted>0.5) = test$y
ifelse(predicted > 0.5,1,0)
ifelse(predicted > 0.5,1,0) == test$y
sum(ifelse(predicted > 0.5,1,0) == test$y)
sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
TESTN <- 10000
sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
for i in 1:TRY{print i}
for (i in 1:TRY){print i}
for(i in 1:TRY){print i}
for(i in 1:10){print i}
for(i in 1:10){}
for(i in 1:10){print(i)}
for(i in 1:TRY){print(i)}
TRY <- 100
TESTN <- 10000
TRAINN <- 100
for(i in 1:TRY){
train <- get_N(TRAINN)
svmfit<-svm(y~.,data=train,kernel="linear",cost=10,scale=FALSE)
test <- get_N(TESTN)
predicted <- predict(svmfit, newdata=test)
result[i] <-  sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
}
result <- vector(length = TRY)
for(i in 1:TRY){
train <- get_N(TRAINN)
svmfit<-svm(y~.,data=train,kernel="linear",cost=10,scale=FALSE)
test <- get_N(TESTN)
predicted <- predict(svmfit, newdata=test)
result[i] <-  sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
}
result
1-mean(result)
sd(result)
result <- vector(length = TRY)
for(i in 1:TRY){
train <- get_N(TRAINN)
svmfit<-svm(y~.,data=train,cost=10,scale=FALSE)
test <- get_N(TESTN)
predicted <- predict(svmfit, newdata=test)
result[i] <-  sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
}
1-mean(result)
sd(result)
result <- vector(length = TRY)
for(i in 1:TRY){
train <- get_N(TRAINN)
svmfit<-svm(y~.,data=train)
test <- get_N(TESTN)
predicted <- predict(svmfit, newdata=test)
result[i] <-  sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
}
1-mean(result)
sd(result)
result <- vector(length = TRY)
for(i in 1:TRY){
train <- get_N(TRAINN)
svmfit<-svm(y~.,data=train)
test <- get_N(TESTN)
predicted <- predict(svmfit, kernel=linear, newdata=test)
result[i] <-  sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
}
1-mean(result)
sd(result)
?gbm
library gbm
library(gbm)
library gbm
?gbm
TRY <- 100
TESTN <- 10000
TRAINN <- 100
?glm
result <- vector(length = TRY)
for(i in 1:TRY){
train <- get_N(TRAINN)
--  ffit<-svm(y~.,data=train)
ffit <- glm(y~.,data=train, family=binomial)
test <- get_N(TESTN)
predicted <- predict(ffit, newdata=test)
result[i] <-  sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
}
1-mean(result)
sd(result)
train <- get_N(TRAINN)
ffit <- glm(y~.,data=train, family=binomial)
summary(ffit)
testmy = test[-y,]
testmy = test[-1,]
head (testmy)
testmy <- test[,-y]
testmy <- test[,-1]
head testmy
head(testmy)
testmy = test[,-1]
predicted <- predict(ffit, newdata=test)
predicted
sum(ifelse(predicted > 0.5,1,0)
)
result <- vector(length = TRY)
for(i in 1:TRY){
train <- get_N(TRAINN)
--  ffit<-svm(y~.,data=train)
ffit <- glm(y~.,data=train, family=binomial)
test <- get_N(TESTN)
testmy = test[,-1]
predicted <- predict(ffit, newdata=test)
result[i] <-  sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
}
1-mean(result)
sd(result)
result
predicted
sum(ifelse(predicted > 0.5,1,0) == test$y)
TRY <- 100
TESTN <- 10000
TRAINN <- 100
result <- vector(length = TRY)
for(i in 1:TRY){
train <- get_N(TRAINN)
--  ffit<-svm(y~.,data=train)
ffit <- glm(y~.,data=train, family=binomial)
test <- get_N(TESTN)
testmy = test[,-1]
predicted <- predict(ffit, newdata=test)
result[i] <-  sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
}
1-mean(result)
sd(result)
result <- vector(length = TRY)
for(i in 1:TRY){
train <- get_N(TRAINN)
--  ffit<-svm(y~.,data=train)
ffit <- glm(y~.,data=train, family=binomial)
test <- get_N(TESTN)
testmy = test[,-1]
predicted <- predict(ffit, newdata=testmy)
result[i] <-  sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
}
1-mean(result)
sd(result)
result
sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
sum(ifelse(predicted > 0.5,1,0) = test$y) / TESTN
result <- array(1:TRY)
for(i in 1:TRY){
train <- get_N(TRAINN)
--  ffit<-svm(y~.,data=train)
ffit <- glm(y~.,data=train, family=binomial)
test <- get_N(TESTN)
testmy = test[,-1]
predicted <- predict(ffit, newdata=testmy)
result[i] <-  sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
}
1-mean(result)
sd(result)
result
result <- array(TRY)
result
a[1] <- 1
a <- array()
a[1] <- 1
a
result <- array()
for(i in 1:TRY){
train <- get_N(TRAINN)
--  ffit<-svm(y~.,data=train)
ffit <- glm(y~.,data=train, family=binomial)
test <- get_N(TESTN)
testmy = test[,-1]
predicted <- predict(ffit, newdata=testmy)
result[i] <-  sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
}
train <- get_N(TRAINN)
--  ffit<-svm(y~.,data=train)
ffit <- glm(y~.,data=train, family=binomial)
ffit <- glm(y~.,data=train, family=binomial())
result <- array()
for(i in 1:TRY){
train <- get_N(TRAINN)
--  ffit<-svm(y~.,data=train)
ffit <- glm(y~.,data=train, family=binomial())
test <- get_N(TESTN)
testmy = test[,-1]
predicted <- predict(ffit, newdata=testmy)
result[i] <-  sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
}
train <- get_N(TRAINN)
--  ffit<-svm(y~.,data=train)
ffit <- glm(y~.,data=train, family=binomial
result <- array()
for(i in 1:TRY){
train <- get_N(TRAINN)
#  ffit<-svm(y~.,data=train)
ffit <- glm(y~.,data=train, family=binomial)
test <- get_N(TESTN)
testmy = test[,-1]
predicted <- predict(ffit, newdata=testmy)
result[i] <-  sum(ifelse(predicted > 0.5,1,0) == test$y) / TESTN
}
1-mean(result)
sd(result)
dimnames(USArrests)
apply(USArrests,2,mean)
apply(USArrests,2, var)
pca.out=prcomp(USArrests, scale=TRUE)
pca.out
names(pca.out)
biplot(pca.out, scale=0)
set.seed(101)
x=matrix(rnorm(100*2),100,2)
xmean=matrix(rnorm(8,sd=4),4,2)
which=sample(1:4,100,replace=TRUE)
x=x+xmean[which,]
plot(x,col=which,pch=19)
km.out=kmeans(x,4,nstart=15)
km.out
plot(x,col=km.out$cluster,cex=2,pch=1,lwd=2)
points(x,col=which,pch=19)
points(x,col=c(4,3,2,1)[which],pch=19)
file <- "C:\\Users\\mmorelli\\Google Drive\\Statistical Learning Stanford\\Week10\\10.R.RData"
load(file)
file <- "C:\\Users\\mmorelli\\Google Drive\\Statistical Learning Stanford\\Week10\\10.R.RData"
load(file)
head(x)
pca.out=prcomp(x, scale=TRUE)
tot <- rbind(x, x.test)
pca.out=prcomp(tot, scale=TRUE)
names(x)
names(x.test)
dimnames(USArrests)
apply(USArrests,2,mean)
apply(USArrests,2, var)
pca.out=prcomp(USArrests, scale=TRUE)
pca.out
names(pca.out)
biplot(pca.out, scale=0)
pca.out=prcomp(tot, scale=TRUE)
?prcomp
pca.out$sdev
pca.out$rotation
pca.out$x
summary(pca.out)
redux <- pca.out$x[1,c(1,5)]
head(redux)
redux <- pca.out$x[1,c(1,5)]
head(redux)
pca.out$sdev
str(pca.out)
str(pca.out)$x
str(pca.out)$x
redux <- pca.out$x[1,c(1,5)]
pca.5 <- pca.out$x[1,c(1,5)]
pca.5 <- pca.out$x[1,c(1:5)]
redux
pca.5 <- pca.out$x[,c(1:5)]
head(pca.5)
pca.5 <- as_data_frame(pca.out$x[,c(1:5)])
pca.5 <- as.data.frame(pca.out$x[,c(1:5)])
reconstructedData<-predict(pca.out, x)
x.pca <-predict(pca.out, x)
head x.pca
x.pca <-predict(pca.out, x)
head(x.pca)
x.pca <-predict(pca.out, x)[,c(1:5)]
head(x.pca)
xy.pca <- cbind(x.pca, y)
head(xy.pca)
fit <- lm(y~., data=xy.pca)
xy.pca <- as.data.frame(cbind(x.pca, y))
head(xy.pca)
fit <- lm(y~., data=xy.pca)
summary(fit)
summary(fit)
x.test.pca  <- predict(pca.out, x.test)[,c(1:5)]
xy.test.pca <- as.data.frame(cbind(x.test.pca, y.test))
y.pred <- predict(fit, xy.test.pca)
se <- (y.pred- y.test)^2
sum(se)/1000
mean(se)
train <- as.data.frame(cbind(x, y))
train <- as.data.frame(cbind(x, y))
test  <- as.data.frame(cbind(x.test, y.test))
fit.all <- lm(y~., train)
y.all.pred <- predict(fit.all, test)
se.all <- (y.all.pred -y.test)^2
mean(se.all)
load(file)
train <- as.data.frame(cbind(x, y))
test  <- as.data.frame(cbind(x.test, y.test))
fit.all <- lm(y~., train)
y.all.pred <- predict(fit.all, test)
se.all <- (y.all.pred -y.test)^2
mean(se.all)
file <- "C:\\Users\\mmorelli\\Google Drive\\Statistical Learning Stanford\\Week10\\10.R.RData"
load(file)
train <- as.data.frame(cbind(x, y))
test  <- as.data.frame(cbind(x.test, y.test))
fit.all <- lm(y~., train)
y.all.pred <- predict(fit.all, test)
se.all <- (y.all.pred -y.test)^2
mean(se.all)
?lm
fit.all
summary(fit.all)
mean(se.all)
file <- "C:\\Users\\mmorelli\\Documents\\work\\Campagne\\10.R.RData\\campagne.csv"
camp <- load.csv(file)
?load
camp <- read.csv(file)
file <- "C:\\Users\\mmorelli\\Documents\\work\\Campagne\\campagne.csv"
camp <- read.csv(file)
head(camp)
names(camp)
?read.csv
camp <- read.csv(file, sep=";")
head(camp)
names(camp)
unique(Subject)
unique(camp$Subject)
names(camp)
unique(camp$Email.Template.Name)
camp[leistung$Email.Template.Name %in% c("2016-01 Contacts Compliance", "2016-01 Mailing Compliance", "2016-01 Leads Compliance"),]$template <- "Compliance"
camp[leistung$Email.Template.Name %in% c("2016-01 Contacts Schadensvermeidung", "2016-01 Leads Schadensvermeidung", "2016-01 Mailing Schadensvermeidung"),]$template <- "Schadensverm."
camp[leistung$Email.Template.Name %in% c("2016-01 Contacts Zeitoptimierung", "2016-01 Mailing Zeitoptimierung", "2016-01 Leads Zeitoptimierung"),]$template <- "Zeitoptim"
camp[leistung$Email.Template.Name %in% c("2016-01 Contacts _berblick", "2016-01 Mailing _berblick", "2016-01 Leads _berblick"),]$template <- ""
camp[leistung$Email.Template.Name %in% c("2016-01 Contacts Planungssicherheit", "2016-01 Mailing Planungssicherheit", "2016-01 Leads Planungssicherheit"),]$template <- ""
camp[camp$Email.Template.Name %in% c("2016-01 Contacts Compliance", "2016-01 Mailing Compliance", "2016-01 Leads Compliance"),]$template <- "Compliance"
camp[camp$Email.Template.Name %in% c("2016-01 Contacts Compliance", "2016-01 Mailing Compliance", "2016-01 Leads Compliance"),]
names(camp)
histogram(camp$OpenedNum)
library(ggplot2)
m <- ggplot(camp, aes(x=OpenedNum))
m + geom_histogram()
str(camp)
m <- ggplot(camp, aes(x=X..Times.Opened))
m + geom_histogram()
m <- ggplot(camp, aes(x=Pattern))
m + geom_histogram()
m <- ggplot(camp, aes(x=X..Times.Opened))
m + geom_histogram()
tail(camp)
camp[is.na(camp[,"Count"]),]
camp[!is.na(camp[,"Count"]),]
camp <- camp[!is.na(camp[,"Count"]),]
m <- ggplot(camp, aes(x=X..Times.Opened))
m + geom_histogram()
?table
names(camp)
table(camp$X..Times.Opened, camp$Type)
table(camp$X..Times.Opened)
table(camp$X..Times.Opened, camp$Type)
names(camp)
table(camp$Wann.Gen, camp$Type)
table(camp[camp$Genervt == 1,]$Wann.Gen, camp[camp$Genervt == 1,]$$Type)
table(camp[camp$Genervt == 1,]$Wann.Gen, camp[camp$Genervt == 1,]$Type)
head(camp)
str(camp)
m <- ggplot(camp, aes(x=X..Times.Opened)) + geom_histogram()
m
table(camp[camp$Genervt == "Ok",]$Wann.Gen, camp[camp$Genervt == "Ok",]$Type)
table(camp[camp$Genervt == "Gen",]$Wann.Gen, camp[camp$Genervt == "Gen",]$Type)
table(camp[camp$Genervt == "Ok",]$Rehienfolge, camp[camp$Genervt == "Ok",]$Type)
str(camp)
table(camp[camp$Genervt == "Ok",]$Rehienfolge, camp[camp$Genervt == "Ok",]$Type)
camp[camp$Genervt == "Ok",]$Rehienfolge
camp[camp$Genervt == "Ok",]$Rehienfolge
camp$Genervt
camp$Genervt == "Ok"
camp[camp$Genervt == "Ok",]
table(camp[camp$Genervt == "Ok",]$Reihenfolge, camp[camp$Genervt == "Ok",]$Type)
file <- "C:\\Users\\mmorelli\\Google Drive\\Statistical Learning Stanford\\Week10\\10.R.RData"
load(file)
head(x)
tot <- rbind(x, x.test)
pca.out=prcomp(tot, scale=TRUE)
pca.out$sdev
redux <- pca.out$x[1,c(1,5)]
dim(redux)
summary(redux)
dimension(redux)
str(redux)
dim.data.frame(redux)
length(redux)
nrow(redux)
redux
redux <- pca.out$x[,c(1,5)]
redux <- pca.out$x[,c(1,5)]
dim(redux)
file <- "C:\\Users\\mmorelli\\Google Drive\\Statistical Learning Stanford\\Week10\\10.R.RData"
load(file)
head(x)
tot <- rbind(x, x.test)
pca.out=prcomp(tot, scale=TRUE)
pca.out$sdev
redux <- pca.out$x[,c(1,5)]
dim(redux)
x.pca  <-predict(pca.out, x)[,c(1:5)]
file <- "C:\\Users\\mmorelli\\Google Drive\\Statistical Learning Stanford\\Week10\\10.R.RData"
load(file)
tot <- rbind(x, x.test)
pca.out=prcomp(tot, scale=TRUE)
file <- "C:\\Users\\mmorelli\\Google Drive\\Statistical Learning Stanford\\Week10\\10.R.RData"
load(file)
tot <- rbind(x, x.test)
pca.out=prcomp(tot, scale=TRUE)
pca.out$sdev
x.pca  <-predict(pca.out, x)[,c(1:5)]
Group<-c(rep("Frank",times=6),rep("Greg",times=11),rep("Stacy",times=3),rep("Nancy",times=10))
X<-c(4,5,3,5,7,4,8,23,4,7,5,2,8,5,8,3,6,5,4,6,8,9,2,5,8,3,6,3,3,4)
Y<-c(7,9,3,6,4,8,7,8,6,3,2,3,6,7,4,6,8,9,5,7,8,9,6,5,4,6,7,8,3,6)
df<-data.frame(Group,as.numeric(X),as.numeric(Y))
Group
df
library(geometry)
install.package(geometry)
install.packages("geometry")
library(geometry)
convhulln(Frank.frame, option="FA")$vol
Frank.frame<-cbind(df$X[df$Group=="Frank"],df$Y[df$Group=="Frank"])
convhulln(Frank.frame, option="FA")$vol
library(tm)
tweets <- read.csv("tweets.csv", stringsAsFactors = F)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
setwd("C:\\Users\\mmorelli\\Google Drive\\MITx 15 071x The Analytics Edge\\Week 07")
Sys.setlocale("LC_ALL", "C")
tweets <- read.csv("tweets.csv", stringsAsFactors = F)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
freq = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(freq))
#2.1
#install.packages("wordcloud")
library(wordcloud)
sort(colSums(allTweets))
colnames(allTweets)
?wordcloud
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(2, 0.25))
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c("apple",stopwords("english")))
freq = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(freq))
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(2, 0.25))
subset(tweets, Avg <=-1)
tweets <- subset(tweets, Avg <=-1)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c("apple",stopwords("english")))
freq = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(freq))
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(2, 0.25))
tweets <- read.csv("tweets.csv", stringsAsFactors = F)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
freq = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(freq))
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(2, 0.25), random.order = F)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c("apple",stopwords("english")))
freq = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(freq))
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(2, 0.25), random.order = F)
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(2, 0.25), rot.per = .5)
library("RColorBrewer")
library(RColorBrewer)
?brewer.pal
display.brewer.all()
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(2, 0.25), min.freq = 10)
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(2, 0.25), min.freq = 10)
display.brewer.all()
?brewer.pal
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(2, 0.25), min.freq = 10, random.color=F, colors=brewer.pal(9, "Blues")[c(-1, -2, -3, -4)] )
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(2, 0.25), min.freq = 10, random.color=T, colors=brewer.pal(9, "Blues")[c(-1, -2, -3, -4)] )
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(2, 0.25), min.freq = 10, random.color=T, colors=brewer.pal(9, "Purples") )
wordcloud(colnames(allTweets), colSums(allTweets), scale=c(2, 0.25), min.freq = 10, random.color=F, colors=brewer.pal(9, "Purples") )
